{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing and visualization with the Fashion MNIST Dataset\n",
    "\n",
    "In this notebook we will be going through the tasks to preprocess the Fashion MNIST dataset and to visualize it. Just like in the previous notebooks, we will be going through a step by step process for the preparation of the dataset for modelling. \n",
    "\n",
    "\n",
    "## Context\n",
    "\n",
    "Why do we need data preprocessing in the first place? Well, the answer is simple. We need to ensure that the data is present with the qualities that we need from it for modelling. What have we discussed so far?\n",
    "\n",
    "1. Visualization helps us understand the data better.\n",
    "2. Removing Erroneous data allows us to pass the data for training without any issues.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "raw_data_train = pd.read_csv(\"FMNIST/fashion-mnist_train.csv\")\n",
    "raw_data_test = pd.read_csv('FMNIST/fashion-mnist_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Visualization\n",
    "\n",
    "### Task 1\n",
    "Lets try one example task. Given our dataset, try to use matplotlib to print out one image in grayscale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Now try to print out 12 images with the labels for the images in a grid form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "For this next task, your goal is to take a subset of 500 images from the dataset and to plot it as a histogram. The X-axis should be values for each pixel and Y-axis should be their frequencies. Use 256 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing\n",
    "\n",
    "Our aim is to create 6 data variables.\n",
    "1. Training set (X, Y)\n",
    "2. Validation set (X, Y)\n",
    "3. Testing set (X, Y)\n",
    "\n",
    "Why do we split the data into training and validation sets?\n",
    "\n",
    "Remember, in the supervised learning problem, we feed our training set (comprises of our selected features and the target) to our learning function. It will then create a model based on the training set.\n",
    "\n",
    "The validation set is subsequently used to test the model's prediction to see how the model performs.\n",
    "\n",
    "For more information, head over to https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6.\n",
    "\n",
    "Do read the section on 'What is Overfitting/Underfitting a Model?' to get more information on why we split test and training sets and its effects if we overfit or underfit a model.\n",
    "\n",
    "Let's implement the code by calling the train_test_split function. We are going to split the train set to be 80% of the data and the validation set to be 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(raw_data_train.iloc[:, 1:])\n",
    "y = pd.get_dummies(np.array(raw_data_train.iloc[:, 0]))\n",
    "\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have extracted the Training, testing and validation data we need to perform certain changes to it. But before we go through with them we need to define the following variables.\n",
    "\n",
    "1. im_rows <-- image rows\n",
    "2. im_cols <-- image columns\n",
    "3. input_shape <-- a tuple containing the input shape for the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_shape variable might be a little tricky to understand. Why is it shaped the way it is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-bc5c17948c59>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-bc5c17948c59>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    The input_shape variable is shaped the way it is to highlight the 3 dimensions an image can have. Since our images are grayscale the images will have dimensions of\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next section you will be challenged to follow the code that we had followed before to create dummies for the 'y_test' variable and create a numpy array for the 'x_test' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task\n",
    "Use the numpy reshaping function to reshape our data to be 4 dimensions. \n",
    "1. Number of images\n",
    "2. Image height\n",
    "3. Image width\n",
    "4. Number of channels(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER\n",
    "# train and validate sets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "The next topic we will be covering is normalization. It is something that is used to get rid of distortion in images and provide a better result in the machine learning process. Upon searching the web for normalization it is easy to get confused by the large number of terms and ideas surrounding it. \n",
    "\n",
    "Check this link for more details https://datascience.stackexchange.com/questions/29958/when-inputting-image-rgb-values-to-mlp-should-i-divide-by-255.\n",
    "\n",
    "### Task\n",
    "\n",
    "Perform normalization for the data variables (X_train, X_validate, X_test) by dividing each value by 255. What does this leave you with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulation!! That is the end of this notebook :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
